version: '3'
services:
  ollama:
    image: ollama/ollama
    container_name: infer-ollama
    ports:
      - 11434:11434
    volumes:
      - ./infer_models:/root/.ollama/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
